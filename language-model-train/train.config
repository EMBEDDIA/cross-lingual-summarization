# INSTRUCTIONS
put preprocessed data into data/tmp_dir
set path to data in 139 and 140 in main script (language_model_char.py)

OPTIONAL: find number of shards

NOTE: maybe you can create vocabulary by yourself and avoid ram problem?

# export model
t2t-exporter \
   --data_dir=$DATA_DIR \
   --problem=$PROBLEM \
   --model=$MODEL \
   --hparams_set=$HPARAMS \
   --output_dir=$OUT_DIR \
   --decode_hparams="beam_size=12,alpha=0.6,batch_size=1,extra_length=512" \
   --t2t_usr_dir=$USR_DIR

# setup env variables
export USR_DIR=./lm/trainer
export PROBLEM=language_model_char
export DATA_DIR=./data/data_dir
export TMP_DIR=./data/tmp_dir
export OUT_DIR=./model
export INF_DIR=./inference
export MODEL=transformer
export HPARAMS=transformer_big_single_gpu

export HPARAMS=transformer_base
export HPARAMS=transformer_big_single_gpu

# generate data
t2t-datagen \
  --t2t_usr_dir=$USR_DIR \
  --problem=$PROBLEM \
  --data_dir=$DATA_DIR \
  --tmp_dir=$TMP_DIR

# train command
t2t-trainer \
  --t2t_usr_dir=$USR_DIR \
  --data_dir=$DATA_DIR \
  --output_dir=$OUT_DIR \
  --problem=$PROBLEM \
  --model=$MODEL \
  --hparams_set=$HPARAMS \
  --hparams="batch_size=2048,max_length=512" \
  --train_steps=1500000 \
  --eval_steps=100 \
  --eval_throttle_seconds=1800 \
  --local_eval_frequency=10000

t2t-trainer --t2t_usr_dir=$USR_DIR --data_dir=$DATA_DIR --output_dir=$OUT_DIR --problem=$PROBLEM --model=$MODEL --hparams_set=$HPARAMS --hparams="batch_size=2048,max_length=512" --train_steps=2000000 --eval_steps=100 --eval_throttle_seconds=1800 --local_eval_frequency=20000
# decoder
export DECODE_FILE=$DATA_DIR/decode-this.txt
export OUTPUT_FILE=./inference/output2.txt
export BEAM_SIZE=12
export ALPHA=0.6
t2t-decoder \
  --t2t_usr_dir=$USR_DIR \
  --data_dir=$DATA_DIR \
  --problem=$PROBLEM \
  --model=$MODEL \
  --hparams_set=$HPARAMS \
  --output_dir=$OUT_DIR \
  --decode_hparams="beam_size=12,alpha=0.6,batch_size=1,extra_length=512" \
  --decode_from_file=$DECODE_FILE \
  --decode_to_file=$OUTPUT_FILE

t2t-decoder \
  --t2t_usr_dir=$USR_DIR \
  --data_dir=$DATA_DIR \
  --problem=$PROBLEM \
  --model=$MODEL \
  --hparams_set=$HPARAMS \
  --output_dir=$OUT_DIR \
  --decode_hparams="extra_length=512,beam_size=12,alpha=0.6,batch_size=1,return_beams=True" \
  --decode_from_file=$DECODE_FILE \
  --decode_to_file=$OUTPUT_FILE \
  --decode_interactive=True

# interactive
t2t-decoder   --t2t_usr_dir=$USR_DIR   --data_dir=$DATA_DIR   --problem=$PROBLEM   --model=$MODEL   --hparams_set=$HPARAMS   --output_dir=$OUT_DIR   --decode_hparams="beam_size=12,alpha=0.6,return_beams=True"   --decode_interactive=True
#########################
trainer default parameters: https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py#L1692
decoder default parameters: https://github.com/tensorflow/tensor2tensor/blob/a0bf3b90b13f75e77fdacf5da025d09309165b92/tensor2tensor/utils/decoding.py#L47


EXPERIMENTS:
export OUTPUT_FILE=./inference/output4.txt
t2t-decoder \
  --t2t_usr_dir=$USR_DIR \
  --data_dir=$DATA_DIR \
  --problem=$PROBLEM \
  --model=$MODEL \
  --hparams_set=$HPARAMS \
  --output_dir=$OUT_DIR \
  --decode_hparams="beam_size=4,alpha=0.6,extra_length=100" \
  --decode_from_file=$DECODE_FILE \
  --decode_to_file=$OUTPUT_FILE

MEANING OF PARAMETERS:
extra_length: https://github.com/tensorflow/tensor2tensor/issues/1289


test model:
mkdir evaluation ?
you will have to generate test set probably with existing code

t2t-datagen \
  --t2t_usr_dir=$USR_DIR \
  --problem=$PROBLEM \
  --data_dir=$DATA_DIR \
  --tmp_dir=$TMP_DIR

t2t-eval \
  --t2t_usr_dir=$USR_DIR \
  --problem=$PROBLEM \
  --model=$MODEL \
  --hparams_set=$HPARAMS \
  --hparams="batch_size=128,max_length=512" \
  --data_dir=$DATA_DIR \
  --output_dir=$OUT_DIR \
  --eval_use_test_set=True \
  --eval_steps=5

Calculate perplexity (e^loss): https://github.com/tensorflow/tensor2tensor/issues/1502

REZULTATI TRENINGA NA PRVIH 30M:

Trainable Variables Total size: 19035136
INFO:tensorflow:Saving dict for global step 900000: global_step = 900000, loss = 1.2723571, metrics-language_model_char/targets/accuracy = 0.82004535, metrics-language_model_char/targets/accuracy_per_sequence = 0.0, metrics-language_model_char/targets/accuracy_top5 = 0.935316, metrics-language_model_char/targets/approx_bleu_score = 0.7191085, metrics-language_model_char/targets/neg_log_perplexity = -0.67224795, metrics-language_model_char/targets/rouge_2_fscore = 0.86345875, metrics-language_model_char/targets/rouge_L_fscore = 0.8031444

PREOSTALI PODATKI
INFO:tensorflow:Saving dict for global step 1500000: global_step = 1500000, loss = 1.2391319, metrics-language_model_char/targets/accuracy = 0.81879854, metrics-language_model_char/targets/accuracy_per_sequence = 0.0, metrics-language_model_char/targets/accuracy_top5 = 0.93223697, metrics-language_model_char/targets/approx_bleu_score = 0.73051584, metrics-language_model_char/targets/neg_log_perplexity = -0.68107784, metrics-language_model_char/targets/rouge_2_fscore = 0.8754265, metrics-language_model_char/targets/rouge_L_fscore = 0.8299711

tensorboard --logdir=path/to/log-directory








t2t-eval \
  --t2t_usr_dir=$USR_DIR \
  --problem=$PROBLEM \
  --model=$MODEL \
  --hparams_set=$HPARAMS \
  --hparams="batch_size=1024,max_length=512" \
  --data_dir=$DATA_DIR \
  --output_dir=$OUT_DIR \
  --eval_use_test_set=True \
  --eval_steps=10


# my computer decode
# setup env variables
export USR_DIR=./lm/trainer
export PROBLEM=language_model_char
export DATA_DIR=./data/data_dir
export TMP_DIR=./data/tmp_dir
export OUT_DIR=./model
export INF_DIR=./inference
export MODEL=transformer
export HPARAMS=transformer_big_single_gpu

t2t-decoder \
  --t2t_usr_dir=$USR_DIR \
  --data_dir=$DATA_DIR \
  --problem=$PROBLEM \
  --model=$MODEL \
  --hparams_set=$HPARAMS \
  --output_dir=$OUT_DIR \
  --decode_hparams="extra_length=512,beam_size=12,alpha=0.6,batch_size=1,return_beams=True" \
  --decode_from_file=$DECODE_FILE \
  --decode_to_file=$OUTPUT_FILE \
  --decode_interactive=True